{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# !pip install pandas\n",
        "# !pip install numpy\n",
        "# !pip install matplotlib\n",
        "# !pip install gensim\n",
        "# !pip install spacy\n",
        "# !pip install nltk\n",
        "# !pip install re\n",
        "# !pip install logging\n",
        "# !pip install warnings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75e8Udntr6qX"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "# !{sys.executable} -m spacy download en\n",
        "import re, numpy as np, pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "# Gensim\n",
        "import gensim, spacy, logging, warnings\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# NLTK Stop words\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n",
        "\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "\n",
        "# Load SpaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
        "\n",
        "# Function to lemmatize the text using SpaCy\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    texts_out = []\n",
        "    for text in texts:\n",
        "        doc = nlp(\" \".join(text))\n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IX1EU-n1sFoC",
        "outputId": "f239cd84-4fa0-4fba-ba94-975aee51154f"
      },
      "outputs": [],
      "source": [
        "#Chargement des données\n",
        "data = pd.read_csv(\"cancer_sport_analyse.csv\")\n",
        "data2 = pd.read_csv(\"cancer_fasting_analyse.csv\")\n",
        "data3 = pd.read_csv(\"cancer_cannabis.csv\")\n",
        "tweets_df = pd.concat([data,data2,data3])\n",
        "\n",
        "tweets_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "cuq2rKNUtrK6",
        "outputId": "fa4f5712-f734-49c2-e83f-df93fdb72fc2"
      },
      "outputs": [],
      "source": [
        "tweets_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "092mzzu9u7f8"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "#Suppression des liens commençant par http et https\n",
        "def remove_url(text):\n",
        "    if isinstance(text, str):\n",
        "        return re.sub(r'https?:\\S*', '', text)\n",
        "    else:\n",
        "        return text\n",
        "tweets_df['Texte corrigé']=tweets_df['Texte corrigé'].apply(remove_url)\n",
        "def remove_url1(text):\n",
        "    if isinstance(text, str):\n",
        "        return re.sub(r'http?:\\S*', '', text)\n",
        "    else:\n",
        "        return text\n",
        "tweets_df['Texte corrigé']=tweets_df['Texte corrigé'].apply(remove_url1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UWoQuGgsMWg",
        "outputId": "9d78471e-f2e5-47ac-d9cd-2be245af771e"
      },
      "outputs": [],
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sent in sentences:\n",
        "        if isinstance(sent, str):\n",
        "            sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
        "            sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
        "            sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
        "            sent = gensim.utils.simple_preprocess(str(sent), deacc=True)\n",
        "            yield(sent)\n",
        "        else:\n",
        "            yield []\n",
        "\n",
        "# Convert to list\n",
        "data = tweets_df['Texte corrigé'].values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7ltzmw1vrZ4"
      },
      "outputs": [],
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "# !python3 -m spacy download en  # run in terminal once\n",
        "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
        "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "    texts = [bigram_mod[doc] for doc in texts]\n",
        "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "    texts_out = []\n",
        "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent))\n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    # remove stopwords once more after lemmatization\n",
        "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]\n",
        "    return texts_out\n",
        "\n",
        "data_ready = process_words(data_words)  # processed Text Data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wu-BOTZLw2O_",
        "outputId": "17ac55dc-18ad-46d9-8e1b-485414c372cd"
      },
      "outputs": [],
      "source": [
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_ready)\n",
        "\n",
        "# Create Corpus: Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
        "\n",
        "# Build LDA model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=8,\n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=10,\n",
        "                                           passes=10,\n",
        "                                           alpha='symmetric',\n",
        "                                           iterations=100,\n",
        "                                           per_word_topics=True)\n",
        "\n",
        "pprint(lda_model.print_topics())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7omeFmszMuS",
        "outputId": "126dde64-455e-4650-abf5-fbc5e4a2c508"
      },
      "outputs": [],
      "source": [
        "# Compute coherence score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_ready, dictionary=id2word)\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('Coherence Score: ', coherence_lda)\n",
        "\n",
        "# Print topics\n",
        "pprint(lda_model.print_topics())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n",
        "    # Init output\n",
        "    sent_topics_df = pd.DataFrame()\n",
        "\n",
        "    # Get main topic in each document\n",
        "    for i, row_list in enumerate(ldamodel[corpus]):\n",
        "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
        "        # print(row)\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # => dominant topic\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                sent_topics_df = pd.concat([sent_topics_df, pd.DataFrame([int(topic_num), round(prop_topic,4), topic_keywords]).T], ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "\n",
        "    # Add original text to the end of the output\n",
        "    contents = pd.Series(texts)\n",
        "    contents = contents.reset_index(drop=True)\n",
        "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
        "    return(sent_topics_df)\n",
        "\n",
        "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready)\n",
        "\n",
        "# Format\n",
        "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
        "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
        "df_dominant_topic.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "4G6s5YBGxDtJ",
        "outputId": "db64d504-d064-44d1-cfb5-5fc91ec52951"
      },
      "outputs": [],
      "source": [
        "# Display setting to show more characters in column\n",
        "pd.options.display.max_colwidth = 100\n",
        "\n",
        "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
        "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
        "\n",
        "for i, grp in sent_topics_outdf_grpd:\n",
        "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet,\n",
        "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)],\n",
        "                                            axis=0)\n",
        "\n",
        "# Reset Index\n",
        "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Format\n",
        "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
        "\n",
        "# Show\n",
        "sent_topics_sorteddf_mallet.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f190_QZlxj9a"
      },
      "outputs": [],
      "source": [
        "# Sentence Coloring of N Sentences\n",
        "def topics_per_document(model, corpus, start=0, end=1):\n",
        "    corpus_sel = corpus[start:end]\n",
        "    dominant_topics = []\n",
        "    topic_percentages = []\n",
        "    for i, corp in enumerate(corpus_sel):\n",
        "        topic_percs, wordid_topics, wordid_phivalues = model[corp]\n",
        "        dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse=True)[0][0]\n",
        "        dominant_topics.append((i, dominant_topic))\n",
        "        topic_percentages.append(topic_percs)\n",
        "    return(dominant_topics, topic_percentages)\n",
        "\n",
        "dominant_topics, topic_percentages = topics_per_document(model=lda_model, corpus=corpus, end=-1)\n",
        "\n",
        "# Distribution of Dominant Topics in Each Document\n",
        "df = pd.DataFrame(dominant_topics, columns=['Document_Id', 'Dominant_Topic'])\n",
        "dominant_topic_in_each_doc = df.groupby('Dominant_Topic').size()\n",
        "df_dominant_topic_in_each_doc = dominant_topic_in_each_doc.to_frame(name='count').reset_index()\n",
        "\n",
        "# Total Topic Distribution by actual weight\n",
        "topic_weightage_by_doc = pd.DataFrame([dict(t) for t in topic_percentages])\n",
        "df_topic_weightage_by_doc = topic_weightage_by_doc.sum().to_frame(name='count').reset_index()\n",
        "\n",
        "# Top 3 Keywords for each Topic\n",
        "topic_top3words = [(i, topic) for i, topics in lda_model.show_topics(formatted=False)\n",
        "                                 for j, (topic, wt) in enumerate(topics) if j < 3]\n",
        "\n",
        "df_top3words_stacked = pd.DataFrame(topic_top3words, columns=['topic_id', 'words'])\n",
        "df_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\n",
        "df_top3words.reset_index(level=0,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "print(pd.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ckaLggJxwya"
      },
      "outputs": [],
      "source": [
        "import pyLDAvis.gensim\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
        "vis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pyLDAvis.save_html(vis, 'lda_visualization.html')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ajouter une nouvelle colonne pour les étiquettes de sujets\n",
        "tweets_df['Topic_Label'] = \"\"\n",
        "\n",
        "# Mapper des noms de sujets aux identifiants de sujets\n",
        "topic_labels_mapping = {\n",
        "    0 : \"Traitement et recherche sur le cancer\",\n",
        "    1 : \"Combat contre le cancer et activisme\",\n",
        "    2 : \"Santé et bien-être liés au cancer\",\n",
        "    3 : \"Études cliniques et croissance tumorale\",\n",
        "    4 : \"Effets des cannabinoïdes sur le cancer\",\n",
        "    5 : \"Utilisation médicale du cannabis dans le traitement du cancer\",\n",
        "    6 : \"Impact environnemental du traitement du cancer\",\n",
        "    7 : \"Potentiel thérapeutique du cannabis dans la lutte contre le cancer\"\n",
        "}\n",
        "\n",
        "# Assigner des étiquettes en fonction des sujets dominants\n",
        "for index, row in df_dominant_topic.iterrows():\n",
        "    tweet_index = row['Document_No']\n",
        "    topic_id = row['Dominant_Topic']\n",
        "    topic_label = topic_labels_mapping[topic_id]\n",
        "    tweets_df.at[tweet_index, 'Topic_Label'] = topic_label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Spécifiez le chemin et le nom de fichier pour sauvegarder le DataFrame avec les étiquettes\n",
        "output_file_path = \"tweets_with_labels.csv\"\n",
        "\n",
        "# Enregistrez le DataFrame avec les étiquettes dans un fichier CSV\n",
        "tweets_df.to_csv(output_file_path, sep=';', index=False)\n",
        "\n",
        "# Confirmez que le fichier a été sauvegardé avec succès\n",
        "print(\"Le DataFrame avec les étiquettes a été sauvegardé dans :\", output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importez pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Lisez le fichier CSV dans un DataFrame\n",
        "t_df = pd.read_csv(\"tweets_with_labels.csv\", sep = \";\", skip_blank_lines=True)\n",
        "# Affichez la forme du DataFrame\n",
        "print(t_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "null_values = t_df[t_df['Topic_Label'].isnull()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "duplicates = t_df[t_df.duplicated(subset=['Topic_Label'], keep=False)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Supprimer les lignes avec des valeurs nulles dans la colonne \"Topic_Label\"\n",
        "#t_df = t_df.dropna(subset=['Topic_Label'])\n",
        "\n",
        "# Supprimer les doublons dans la colonne \"Topic_Label\"\n",
        "t_df = t_df.drop_duplicates(subset=['Fichier'], keep='first')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(t_df.shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
