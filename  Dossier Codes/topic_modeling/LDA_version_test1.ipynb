{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.1)\n",
            "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.2)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (10.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: gensim in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (1.26.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (6.4.0)\n",
            "Requirement already satisfied: spacy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.6.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (69.1.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.26.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: nltk in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: click in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2023.10.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement re (from versions: none)\n",
            "ERROR: No matching distribution found for re\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting logging\n",
            "  Downloading logging-0.4.9.6.tar.gz (96 kB)\n",
            "     ---------------------------------------- 0.0/96.0 kB ? eta -:--:--\n",
            "     ---- ----------------------------------- 10.2/96.0 kB ? eta -:--:--\n",
            "     ---------------------------- --------- 71.7/96.0 kB 653.6 kB/s eta 0:00:01\n",
            "     -------------------------------------- 96.0/96.0 kB 779.9 kB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'error'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × python setup.py egg_info did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [24 lines of output]\n",
            "      Traceback (most recent call last):\n",
            "        File \"<string>\", line 2, in <module>\n",
            "        File \"<pip-setuptools-caller>\", line 14, in <module>\n",
            "        File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\setuptools\\__init__.py\", line 7, in <module>\n",
            "          import _distutils_hack.override  # noqa: F401\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "        File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\_distutils_hack\\override.py\", line 1, in <module>\n",
            "          __import__('_distutils_hack').do_override()\n",
            "        File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\_distutils_hack\\__init__.py\", line 70, in do_override\n",
            "          ensure_local_distutils()\n",
            "        File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\_distutils_hack\\__init__.py\", line 56, in ensure_local_distutils\n",
            "          core = importlib.import_module('distutils.core')\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "        File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
            "          return _bootstrap._gcd_import(name[level:], package, level)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "        File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\setuptools\\_distutils\\core.py\", line 22, in <module>\n",
            "          from .dist import Distribution\n",
            "        File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 12, in <module>\n",
            "          import logging\n",
            "        File \"C:\\Users\\User\\AppData\\Local\\Temp\\pip-install-o2yu051a\\logging_2e69a56397b946a5a5ecfd6ad3a3e4aa\\logging\\__init__.py\", line 618\n",
            "          raise NotImplementedError, 'emit must be implemented '\\\n",
            "                                   ^\n",
            "      SyntaxError: invalid syntax\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "error: metadata-generation-failed\n",
            "\n",
            "× Encountered error while generating package metadata.\n",
            "╰─> See above for output.\n",
            "\n",
            "note: This is an issue with the package mentioned above, not pip.\n",
            "hint: See above for details.\n",
            "ERROR: Could not find a version that satisfies the requirement warnings (from versions: none)\n",
            "ERROR: No matching distribution found for warnings\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%pip install pandas\n",
        "%pip install numpy\n",
        "%pip install matplotlib\n",
        "%pip install gensim\n",
        "%pip install spacy\n",
        "%pip install nltk\n",
        "%pip install re\n",
        "%pip install logging\n",
        "%pip install warnings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "75e8Udntr6qX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "# !{sys.executable} -m spacy download en\n",
        "import re, numpy as np, pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "# Gensim\n",
        "import gensim, spacy, logging, warnings\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# NLTK Stop words\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n",
        "\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "\n",
        "# Load SpaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
        "\n",
        "# Function to lemmatize the text using SpaCy\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    texts_out = []\n",
        "    for text in texts:\n",
        "        doc = nlp(\" \".join(text))\n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IX1EU-n1sFoC",
        "outputId": "f239cd84-4fa0-4fba-ba94-975aee51154f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(52916, 17)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Chargement des données\n",
        "data = pd.read_csv(\"../../Donnees/analyse_texte/csv/cancer_sport_analyse.csv\")\n",
        "data2 = pd.read_csv(\"../../Donnees/analyse_texte/csv/cancer_fasting_analyse.csv\")\n",
        "data3 = pd.read_csv(\"../../Donnees/analyse_texte/csv/cancer_cannabis.csv\")\n",
        "tweets_df = pd.concat([data,data2,data3])\n",
        "\n",
        "tweets_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "cuq2rKNUtrK6",
        "outputId": "fa4f5712-f734-49c2-e83f-df93fdb72fc2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Fichier</th>\n",
              "      <th>ID</th>\n",
              "      <th>Nb retweet</th>\n",
              "      <th>Nb like</th>\n",
              "      <th>Nb réponses</th>\n",
              "      <th>Nb citations</th>\n",
              "      <th>Hashtags</th>\n",
              "      <th>Texte</th>\n",
              "      <th>Mots</th>\n",
              "      <th>Phrases</th>\n",
              "      <th>Tags POS</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Polarité</th>\n",
              "      <th>Subjectivité</th>\n",
              "      <th>Phrases nominales</th>\n",
              "      <th>Texte corrigé</th>\n",
              "      <th>Texte tokénizé</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>721916579533950976.json</td>\n",
              "      <td>721916579533950976</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>['#CANCER', '#SPORT', '#PARENTING', '#SCHOOL',...</td>\n",
              "      <td>Is artificial turf giving athletes #cancer ? -...</td>\n",
              "      <td>['Is', 'artificial', 'turf', 'giving', 'athlet...</td>\n",
              "      <td>[Sentence(\"Is artificial turf giving athletes ...</td>\n",
              "      <td>[('Is', 'VBZ'), ('artificial', 'JJ'), ('turf',...</td>\n",
              "      <td>Sentiment(polarity=-0.6, subjectivity=1.0)</td>\n",
              "      <td>-0.6</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>['artificial turf', 'athletes # cancer', 'spor...</td>\n",
              "      <td>Is artificial turf giving athletes #cancer ? -...</td>\n",
              "      <td>['Is', 'artificial', 'turf', 'giving', 'athlet...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>238304774825603072.json</td>\n",
              "      <td>238304774825603072</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>['#CANCER', '#SPORT']</td>\n",
              "      <td>Frankel's victory at York made me feel '20 yea...</td>\n",
              "      <td>['Frankel', \"'s\", 'victory', 'at', 'York', 'ma...</td>\n",
              "      <td>[Sentence(\"Frankel's victory at York made me f...</td>\n",
              "      <td>[('Frankel', 'NNP'), (\"'s\", 'POS'), ('victory'...</td>\n",
              "      <td>Sentiment(polarity=0.5, subjectivity=0.5)</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5000</td>\n",
              "      <td>['frankel', \"'s victory\", 'york', \"'20 years\",...</td>\n",
              "      <td>Fraenkel's victory at Work made me feel '20 ye...</td>\n",
              "      <td>['Frankel', \"'s\", 'victory', 'at', 'York', 'ma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1007390700726575104.json</td>\n",
              "      <td>1007390700726575104</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>['#SPORT', '#CANCER']</td>\n",
              "      <td>READ THIS! What an absolutely brilliant piece ...</td>\n",
              "      <td>['READ', 'THIS', 'What', 'an', 'absolutely', '...</td>\n",
              "      <td>[Sentence(\"READ THIS!\"), Sentence(\"What an abs...</td>\n",
              "      <td>[('READ', 'VB'), ('THIS', 'NN'), ('What', 'WP'...</td>\n",
              "      <td>Sentiment(polarity=0.5, subjectivity=0.5375)</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5375</td>\n",
              "      <td>['read this', 'brilliant piece', '# sport trib...</td>\n",
              "      <td>READ THIS! That an absolutely brilliant piece ...</td>\n",
              "      <td>['READ', 'THIS', '!', 'What', 'an', 'absolutel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>668965073889697795.json</td>\n",
              "      <td>668965073889697795</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>['#CANCER', '#STROKE', '#SPORT']</td>\n",
              "      <td>MJA InSight News: #Cancer survivors, exercise ...</td>\n",
              "      <td>['MJA', 'InSight', 'News', 'Cancer', 'survivor...</td>\n",
              "      <td>[Sentence(\"MJA InSight News: #Cancer survivors...</td>\n",
              "      <td>[('MJA', 'NNP'), ('InSight', 'NNP'), ('News', ...</td>\n",
              "      <td>Sentiment(polarity=0.0, subjectivity=0.0)</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>['mja insight', '# cancer survivors', 'atopic ...</td>\n",
              "      <td>MJA night News: #Cancer survivor, exercise bar...</td>\n",
              "      <td>['MJA', 'InSight', 'News', ':', '#', 'Cancer',...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1098650711049555968.json</td>\n",
              "      <td>1098650711049555968</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>['#HEALTH', '#CANCER', '#SPORT']</td>\n",
              "      <td>When #health #cancer &amp;amp; #sport are inextric...</td>\n",
              "      <td>['When', 'health', 'cancer', 'amp', 'sport', '...</td>\n",
              "      <td>[Sentence(\"When #health #cancer &amp;amp; #sport a...</td>\n",
              "      <td>[('When', 'WRB'), ('health', 'NN'), ('cancer',...</td>\n",
              "      <td>Sentiment(polarity=0.6000000000000001, subject...</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.9000</td>\n",
              "      <td>['# health # cancer', '# sport', 'amazing', 'd...</td>\n",
              "      <td>When #health #cancer &amp;amp; #sport are inextric...</td>\n",
              "      <td>['When', '#', 'health', '#', 'cancer', '&amp;', 'a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    Fichier                   ID Nb retweet Nb like  \\\n",
              "0   721916579533950976.json   721916579533950976          0       0   \n",
              "1   238304774825603072.json   238304774825603072          0       1   \n",
              "2  1007390700726575104.json  1007390700726575104          0       4   \n",
              "3   668965073889697795.json   668965073889697795          0       0   \n",
              "4  1098650711049555968.json  1098650711049555968          2       1   \n",
              "\n",
              "  Nb réponses  Nb citations  \\\n",
              "0           0           0.0   \n",
              "1           0           0.0   \n",
              "2           1           0.0   \n",
              "3           0           0.0   \n",
              "4           0           0.0   \n",
              "\n",
              "                                            Hashtags  \\\n",
              "0  ['#CANCER', '#SPORT', '#PARENTING', '#SCHOOL',...   \n",
              "1                              ['#CANCER', '#SPORT']   \n",
              "2                              ['#SPORT', '#CANCER']   \n",
              "3                   ['#CANCER', '#STROKE', '#SPORT']   \n",
              "4                   ['#HEALTH', '#CANCER', '#SPORT']   \n",
              "\n",
              "                                               Texte  \\\n",
              "0  Is artificial turf giving athletes #cancer ? -...   \n",
              "1  Frankel's victory at York made me feel '20 yea...   \n",
              "2  READ THIS! What an absolutely brilliant piece ...   \n",
              "3  MJA InSight News: #Cancer survivors, exercise ...   \n",
              "4  When #health #cancer &amp; #sport are inextric...   \n",
              "\n",
              "                                                Mots  \\\n",
              "0  ['Is', 'artificial', 'turf', 'giving', 'athlet...   \n",
              "1  ['Frankel', \"'s\", 'victory', 'at', 'York', 'ma...   \n",
              "2  ['READ', 'THIS', 'What', 'an', 'absolutely', '...   \n",
              "3  ['MJA', 'InSight', 'News', 'Cancer', 'survivor...   \n",
              "4  ['When', 'health', 'cancer', 'amp', 'sport', '...   \n",
              "\n",
              "                                             Phrases  \\\n",
              "0  [Sentence(\"Is artificial turf giving athletes ...   \n",
              "1  [Sentence(\"Frankel's victory at York made me f...   \n",
              "2  [Sentence(\"READ THIS!\"), Sentence(\"What an abs...   \n",
              "3  [Sentence(\"MJA InSight News: #Cancer survivors...   \n",
              "4  [Sentence(\"When #health #cancer &amp; #sport a...   \n",
              "\n",
              "                                            Tags POS  \\\n",
              "0  [('Is', 'VBZ'), ('artificial', 'JJ'), ('turf',...   \n",
              "1  [('Frankel', 'NNP'), (\"'s\", 'POS'), ('victory'...   \n",
              "2  [('READ', 'VB'), ('THIS', 'NN'), ('What', 'WP'...   \n",
              "3  [('MJA', 'NNP'), ('InSight', 'NNP'), ('News', ...   \n",
              "4  [('When', 'WRB'), ('health', 'NN'), ('cancer',...   \n",
              "\n",
              "                                           Sentiment  Polarité  Subjectivité  \\\n",
              "0         Sentiment(polarity=-0.6, subjectivity=1.0)      -0.6        1.0000   \n",
              "1          Sentiment(polarity=0.5, subjectivity=0.5)       0.5        0.5000   \n",
              "2       Sentiment(polarity=0.5, subjectivity=0.5375)       0.5        0.5375   \n",
              "3          Sentiment(polarity=0.0, subjectivity=0.0)       0.0        0.0000   \n",
              "4  Sentiment(polarity=0.6000000000000001, subject...       0.6        0.9000   \n",
              "\n",
              "                                   Phrases nominales  \\\n",
              "0  ['artificial turf', 'athletes # cancer', 'spor...   \n",
              "1  ['frankel', \"'s victory\", 'york', \"'20 years\",...   \n",
              "2  ['read this', 'brilliant piece', '# sport trib...   \n",
              "3  ['mja insight', '# cancer survivors', 'atopic ...   \n",
              "4  ['# health # cancer', '# sport', 'amazing', 'd...   \n",
              "\n",
              "                                       Texte corrigé  \\\n",
              "0  Is artificial turf giving athletes #cancer ? -...   \n",
              "1  Fraenkel's victory at Work made me feel '20 ye...   \n",
              "2  READ THIS! That an absolutely brilliant piece ...   \n",
              "3  MJA night News: #Cancer survivor, exercise bar...   \n",
              "4  When #health #cancer &amp; #sport are inextric...   \n",
              "\n",
              "                                      Texte tokénizé  \n",
              "0  ['Is', 'artificial', 'turf', 'giving', 'athlet...  \n",
              "1  ['Frankel', \"'s\", 'victory', 'at', 'York', 'ma...  \n",
              "2  ['READ', 'THIS', '!', 'What', 'an', 'absolutel...  \n",
              "3  ['MJA', 'InSight', 'News', ':', '#', 'Cancer',...  \n",
              "4  ['When', '#', 'health', '#', 'cancer', '&', 'a...  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tweets_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "092mzzu9u7f8"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "#Suppression des liens commençant par http et https\n",
        "def remove_url(text):\n",
        "    if isinstance(text, str):\n",
        "        return re.sub(r'https?:\\S*', '', text)\n",
        "    else:\n",
        "        return text\n",
        "tweets_df['Texte corrigé']=tweets_df['Texte corrigé'].apply(remove_url)\n",
        "def remove_url1(text):\n",
        "    if isinstance(text, str):\n",
        "        return re.sub(r'http?:\\S*', '', text)\n",
        "    else:\n",
        "        return text\n",
        "tweets_df['Texte corrigé']=tweets_df['Texte corrigé'].apply(remove_url1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UWoQuGgsMWg",
        "outputId": "9d78471e-f2e5-47ac-d9cd-2be245af771e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:4: SyntaxWarning: invalid escape sequence '\\S'\n",
            "<>:5: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:4: SyntaxWarning: invalid escape sequence '\\S'\n",
            "<>:5: SyntaxWarning: invalid escape sequence '\\s'\n",
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_23280\\166417147.py:4: SyntaxWarning: invalid escape sequence '\\S'\n",
            "  sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_23280\\166417147.py:5: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['is', 'artificial', 'turf', 'giving', 'athletes', 'cancer', 'port', 'printing', 'school', 'turf', 'grass', 'lawn']]\n"
          ]
        }
      ],
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sent in sentences:\n",
        "        if isinstance(sent, str):\n",
        "            sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
        "            sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
        "            sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
        "            sent = gensim.utils.simple_preprocess(str(sent), deacc=True)\n",
        "            yield(sent)\n",
        "        else:\n",
        "            yield []\n",
        "\n",
        "# Convert to list\n",
        "data = tweets_df['Texte corrigé'].values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "l7ltzmw1vrZ4"
      },
      "outputs": [],
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "# !python3 -m spacy download en  # run in terminal once\n",
        "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
        "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "    texts = [bigram_mod[doc] for doc in texts]\n",
        "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "    texts_out = []\n",
        "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent))\n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    # remove stopwords once more after lemmatization\n",
        "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]\n",
        "    return texts_out\n",
        "\n",
        "data_ready = process_words(data_words)  # processed Text Data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wu-BOTZLw2O_",
        "outputId": "17ac55dc-18ad-46d9-8e1b-485414c372cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0,\n",
            "  '0.106*\"grow\" + 0.105*\"thing\" + 0.077*\"free\" + 0.059*\"start\" + 0.058*\"legal\" '\n",
            "  '+ 0.056*\"long\" + 0.053*\"illegal\" + 0.043*\"trial\" + 0.040*\"happy\" + '\n",
            "  '0.029*\"first\"'),\n",
            " (1,\n",
            "  '0.000*\"hang\" + 0.000*\"association\" + 0.000*\"ebb\" + 0.000*\"ply_watch\" + '\n",
            "  '0.000*\"force\" + 0.000*\"fool\" + 0.000*\"thcnation\" + 0.000*\"lead\" + '\n",
            "  '0.000*\"let_parma\" + 0.000*\"drops_daily\"'),\n",
            " (2,\n",
            "  '0.252*\"cancer\" + 0.140*\"cure\" + 0.109*\"medical\" + 0.053*\"health\" + '\n",
            "  '0.049*\"sure\" + 0.032*\"medicine\" + 0.024*\"tumour\" + 0.022*\"treat\" + '\n",
            "  '0.017*\"watch\" + 0.017*\"benefit\"'),\n",
            " (3,\n",
            "  '0.154*\"cancer\" + 0.142*\"study\" + 0.077*\"treatment\" + 0.063*\"thc\" + '\n",
            "  '0.048*\"tumor\" + 0.048*\"people\" + 0.035*\"brain\" + 0.032*\"reduce\" + '\n",
            "  '0.029*\"mmj\" + 0.028*\"show\"'),\n",
            " (4,\n",
            "  '0.078*\"cancer\" + 0.058*\"great\" + 0.052*\"drug\" + 0.049*\"plant\" + '\n",
            "  '0.035*\"time\" + 0.033*\"die\" + 0.032*\"extract\" + 0.030*\"check\" + '\n",
            "  '0.029*\"chemotherapy\" + 0.028*\"food\"'),\n",
            " (5,\n",
            "  '0.000*\"hang\" + 0.000*\"association\" + 0.000*\"ebb\" + 0.000*\"ply_watch\" + '\n",
            "  '0.000*\"force\" + 0.000*\"fool\" + 0.000*\"thcnation\" + 0.000*\"lead\" + '\n",
            "  '0.000*\"let_parma\" + 0.000*\"drops_daily\"'),\n",
            " (6,\n",
            "  '0.000*\"hang\" + 0.000*\"association\" + 0.000*\"ebb\" + 0.000*\"ply_watch\" + '\n",
            "  '0.000*\"force\" + 0.000*\"fool\" + 0.000*\"thcnation\" + 0.000*\"lead\" + '\n",
            "  '0.000*\"let_parma\" + 0.000*\"drops_daily\"'),\n",
            " (7,\n",
            "  '0.000*\"hang\" + 0.000*\"association\" + 0.000*\"ebb\" + 0.000*\"ply_watch\" + '\n",
            "  '0.000*\"force\" + 0.000*\"fool\" + 0.000*\"thcnation\" + 0.000*\"lead\" + '\n",
            "  '0.000*\"let_parma\" + 0.000*\"drops_daily\"'),\n",
            " (8,\n",
            "  '0.329*\"cancer\" + 0.157*\"patient\" + 0.048*\"mme\" + 0.042*\"find\" + '\n",
            "  '0.027*\"life\" + 0.027*\"journey\" + 0.027*\"legalize\" + 0.024*\"meet\" + '\n",
            "  '0.023*\"science\" + 0.020*\"become\"'),\n",
            " (9,\n",
            "  '0.000*\"hang\" + 0.000*\"association\" + 0.000*\"ebb\" + 0.000*\"ply_watch\" + '\n",
            "  '0.000*\"force\" + 0.000*\"fool\" + 0.000*\"thcnation\" + 0.000*\"lead\" + '\n",
            "  '0.000*\"let_parma\" + 0.000*\"drops_daily\"'),\n",
            " (10,\n",
            "  '0.276*\"cancer\" + 0.120*\"cannabinoid\" + 0.072*\"cell\" + 0.048*\"kill\" + '\n",
            "  '0.048*\"child\" + 0.046*\"research\" + 0.042*\"growth\" + 0.040*\"pot\" + '\n",
            "  '0.026*\"woman\" + 0.025*\"inhibit\"'),\n",
            " (11,\n",
            "  '0.198*\"help\" + 0.129*\"cancer\" + 0.076*\"day\" + 0.069*\"new\" + '\n",
            "  '0.066*\"cannabisoil\" + 0.040*\"stop\" + 0.039*\"fight\" + 0.038*\"amp\" + '\n",
            "  '0.038*\"rush\" + 0.027*\"breakthetaboo\"'),\n",
            " (12,\n",
            "  '0.260*\"cannabis\" + 0.198*\"cancer\" + 0.072*\"cod\" + 0.066*\"oil\" + '\n",
            "  '0.052*\"despoil\" + 0.038*\"pain\" + 0.026*\"cbd\" + 0.020*\"weed\" + '\n",
            "  '0.018*\"marijuana\" + 0.017*\"epilepsy\"'),\n",
            " (13,\n",
            "  '0.000*\"hang\" + 0.000*\"association\" + 0.000*\"ebb\" + 0.000*\"ply_watch\" + '\n",
            "  '0.000*\"force\" + 0.000*\"fool\" + 0.000*\"thcnation\" + 0.000*\"lead\" + '\n",
            "  '0.000*\"let_parma\" + 0.000*\"drops_daily\"'),\n",
            " (14,\n",
            "  '0.000*\"hang\" + 0.000*\"association\" + 0.000*\"ebb\" + 0.000*\"ply_watch\" + '\n",
            "  '0.000*\"force\" + 0.000*\"fool\" + 0.000*\"thcnation\" + 0.000*\"lead\" + '\n",
            "  '0.000*\"let_parma\" + 0.000*\"drops_daily\"')]\n"
          ]
        }
      ],
      "source": [
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_ready)\n",
        "\n",
        "# Create Corpus: Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
        "\n",
        "# Build LDA model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=15,\n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=10,\n",
        "                                           passes=10,\n",
        "                                           alpha='symmetric',\n",
        "                                           iterations=100)\n",
        "\n",
        "pprint(lda_model.print_topics())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7omeFmszMuS",
        "outputId": "126dde64-455e-4650-abf5-fbc5e4a2c508"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coherence Score:  0.44431329662619207\n",
            "[(0,\n",
            "  '0.106*\"grow\" + 0.105*\"thing\" + 0.077*\"free\" + 0.059*\"start\" + 0.058*\"legal\" '\n",
            "  '+ 0.056*\"long\" + 0.053*\"illegal\" + 0.043*\"trial\" + 0.040*\"happy\" + '\n",
            "  '0.029*\"first\"'),\n",
            " (1,\n",
            "  '0.000*\"hang\" + 0.000*\"association\" + 0.000*\"ebb\" + 0.000*\"ply_watch\" + '\n",
            "  '0.000*\"force\" + 0.000*\"fool\" + 0.000*\"thcnation\" + 0.000*\"lead\" + '\n",
            "  '0.000*\"let_parma\" + 0.000*\"drops_daily\"'),\n",
            " (2,\n",
            "  '0.252*\"cancer\" + 0.140*\"cure\" + 0.109*\"medical\" + 0.053*\"health\" + '\n",
            "  '0.049*\"sure\" + 0.032*\"medicine\" + 0.024*\"tumour\" + 0.022*\"treat\" + '\n",
            "  '0.017*\"watch\" + 0.017*\"benefit\"'),\n",
            " (3,\n",
            "  '0.154*\"cancer\" + 0.142*\"study\" + 0.077*\"treatment\" + 0.063*\"thc\" + '\n",
            "  '0.048*\"tumor\" + 0.048*\"people\" + 0.035*\"brain\" + 0.032*\"reduce\" + '\n",
            "  '0.029*\"mmj\" + 0.028*\"show\"'),\n",
            " (4,\n",
            "  '0.078*\"cancer\" + 0.058*\"great\" + 0.052*\"drug\" + 0.049*\"plant\" + '\n",
            "  '0.035*\"time\" + 0.033*\"die\" + 0.032*\"extract\" + 0.030*\"check\" + '\n",
            "  '0.029*\"chemotherapy\" + 0.028*\"food\"'),\n",
            " (5,\n",
            "  '0.000*\"hang\" + 0.000*\"association\" + 0.000*\"ebb\" + 0.000*\"ply_watch\" + '\n",
            "  '0.000*\"force\" + 0.000*\"fool\" + 0.000*\"thcnation\" + 0.000*\"lead\" + '\n",
            "  '0.000*\"let_parma\" + 0.000*\"drops_daily\"'),\n",
            " (6,\n",
            "  '0.000*\"hang\" + 0.000*\"association\" + 0.000*\"ebb\" + 0.000*\"ply_watch\" + '\n",
            "  '0.000*\"force\" + 0.000*\"fool\" + 0.000*\"thcnation\" + 0.000*\"lead\" + '\n",
            "  '0.000*\"let_parma\" + 0.000*\"drops_daily\"'),\n",
            " (7,\n",
            "  '0.000*\"hang\" + 0.000*\"association\" + 0.000*\"ebb\" + 0.000*\"ply_watch\" + '\n",
            "  '0.000*\"force\" + 0.000*\"fool\" + 0.000*\"thcnation\" + 0.000*\"lead\" + '\n",
            "  '0.000*\"let_parma\" + 0.000*\"drops_daily\"'),\n",
            " (8,\n",
            "  '0.329*\"cancer\" + 0.157*\"patient\" + 0.048*\"mme\" + 0.042*\"find\" + '\n",
            "  '0.027*\"life\" + 0.027*\"journey\" + 0.027*\"legalize\" + 0.024*\"meet\" + '\n",
            "  '0.023*\"science\" + 0.020*\"become\"'),\n",
            " (9,\n",
            "  '0.000*\"hang\" + 0.000*\"association\" + 0.000*\"ebb\" + 0.000*\"ply_watch\" + '\n",
            "  '0.000*\"force\" + 0.000*\"fool\" + 0.000*\"thcnation\" + 0.000*\"lead\" + '\n",
            "  '0.000*\"let_parma\" + 0.000*\"drops_daily\"'),\n",
            " (10,\n",
            "  '0.276*\"cancer\" + 0.120*\"cannabinoid\" + 0.072*\"cell\" + 0.048*\"kill\" + '\n",
            "  '0.048*\"child\" + 0.046*\"research\" + 0.042*\"growth\" + 0.040*\"pot\" + '\n",
            "  '0.026*\"woman\" + 0.025*\"inhibit\"'),\n",
            " (11,\n",
            "  '0.198*\"help\" + 0.129*\"cancer\" + 0.076*\"day\" + 0.069*\"new\" + '\n",
            "  '0.066*\"cannabisoil\" + 0.040*\"stop\" + 0.039*\"fight\" + 0.038*\"amp\" + '\n",
            "  '0.038*\"rush\" + 0.027*\"breakthetaboo\"'),\n",
            " (12,\n",
            "  '0.260*\"cannabis\" + 0.198*\"cancer\" + 0.072*\"cod\" + 0.066*\"oil\" + '\n",
            "  '0.052*\"despoil\" + 0.038*\"pain\" + 0.026*\"cbd\" + 0.020*\"weed\" + '\n",
            "  '0.018*\"marijuana\" + 0.017*\"epilepsy\"'),\n",
            " (13,\n",
            "  '0.000*\"hang\" + 0.000*\"association\" + 0.000*\"ebb\" + 0.000*\"ply_watch\" + '\n",
            "  '0.000*\"force\" + 0.000*\"fool\" + 0.000*\"thcnation\" + 0.000*\"lead\" + '\n",
            "  '0.000*\"let_parma\" + 0.000*\"drops_daily\"'),\n",
            " (14,\n",
            "  '0.000*\"hang\" + 0.000*\"association\" + 0.000*\"ebb\" + 0.000*\"ply_watch\" + '\n",
            "  '0.000*\"force\" + 0.000*\"fool\" + 0.000*\"thcnation\" + 0.000*\"lead\" + '\n",
            "  '0.000*\"let_parma\" + 0.000*\"drops_daily\"')]\n"
          ]
        }
      ],
      "source": [
        "# Compute coherence score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_ready, dictionary=id2word)\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('Coherence Score: ', coherence_lda)\n",
        "\n",
        "# Print topics\n",
        "pprint(lda_model.print_topics())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Document_No</th>\n",
              "      <th>Dominant_Topic</th>\n",
              "      <th>Topic_Perc_Contrib</th>\n",
              "      <th>Keywords</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>0.6889</td>\n",
              "      <td>help, cancer, day, new, cannabisoil, stop, fight, amp, rush, breakthetaboo</td>\n",
              "      <td>[artificial, turf, give, athlete, cancer, port, print, school, turf, lawn]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>0.8667</td>\n",
              "      <td>cannabis, cancer, cod, oil, despoil, pain, cbd, weed, marijuana, epilepsy</td>\n",
              "      <td>[fraenkel, victory, work, feel, year, well, trainer, air, cecil, battle, cancer, port]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0.7398</td>\n",
              "      <td>cancer, great, drug, plant, time, die, extract, check, chemotherapy, food</td>\n",
              "      <td>[read, absolutely, brilliant, piece, sport, tribe, survive, cancer, suffer, rude, honor, privile...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.8133</td>\n",
              "      <td>cancer, study, treatment, thc, tumor, people, brain, reduce, mmj, show</td>\n",
              "      <td>[night, news, cancer, survivor, exercise, barrier, topic, dermatitis, bilingualism, stroke, spor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0.6587</td>\n",
              "      <td>cancer, cure, medical, health, sure, medicine, tumour, treat, watch, benefit</td>\n",
              "      <td>[health, cancer, sport, inextricably, link, amazing, youngster, still, stay, play, soccer, diagn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>11</td>\n",
              "      <td>0.5653</td>\n",
              "      <td>help, cancer, day, new, cannabisoil, stop, fight, amp, rush, breakthetaboo</td>\n",
              "      <td>[help, honor, step, cancer, journey, give, hope, lady, cancer, event, dubaiculture]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>0.6889</td>\n",
              "      <td>cancer, cannabinoid, cell, kill, child, research, growth, pot, woman, inhibit</td>\n",
              "      <td>[charity_ruin, cancer, research, baffle]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>12</td>\n",
              "      <td>0.8833</td>\n",
              "      <td>cannabis, cancer, cod, oil, despoil, pain, cbd, weed, marijuana, epilepsy</td>\n",
              "      <td>[oil, list, usa_drug_list, ff, cancer, cod, pain, sport]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "      <td>0.5739</td>\n",
              "      <td>cancer, cannabinoid, cell, kill, child, research, growth, pot, woman, inhibit</td>\n",
              "      <td>[cheshirehour, look, charity, work, cancer, child, sport, contact]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "      <td>0.7667</td>\n",
              "      <td>help, cancer, day, new, cannabisoil, stop, fight, amp, rush, breakthetaboo</td>\n",
              "      <td>[seat, solution, bad, condition, train, give, sport, charity, cancer, fundraising]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Document_No Dominant_Topic Topic_Perc_Contrib  \\\n",
              "0            0             11             0.6889   \n",
              "1            1             12             0.8667   \n",
              "2            2              4             0.7398   \n",
              "3            3              3             0.8133   \n",
              "4            4              2             0.6587   \n",
              "5            5             11             0.5653   \n",
              "6            6             10             0.6889   \n",
              "7            7             12             0.8833   \n",
              "8            8             10             0.5739   \n",
              "9            9             11             0.7667   \n",
              "\n",
              "                                                                        Keywords  \\\n",
              "0     help, cancer, day, new, cannabisoil, stop, fight, amp, rush, breakthetaboo   \n",
              "1      cannabis, cancer, cod, oil, despoil, pain, cbd, weed, marijuana, epilepsy   \n",
              "2      cancer, great, drug, plant, time, die, extract, check, chemotherapy, food   \n",
              "3         cancer, study, treatment, thc, tumor, people, brain, reduce, mmj, show   \n",
              "4   cancer, cure, medical, health, sure, medicine, tumour, treat, watch, benefit   \n",
              "5     help, cancer, day, new, cannabisoil, stop, fight, amp, rush, breakthetaboo   \n",
              "6  cancer, cannabinoid, cell, kill, child, research, growth, pot, woman, inhibit   \n",
              "7      cannabis, cancer, cod, oil, despoil, pain, cbd, weed, marijuana, epilepsy   \n",
              "8  cancer, cannabinoid, cell, kill, child, research, growth, pot, woman, inhibit   \n",
              "9     help, cancer, day, new, cannabisoil, stop, fight, amp, rush, breakthetaboo   \n",
              "\n",
              "                                                                                                  Text  \n",
              "0                           [artificial, turf, give, athlete, cancer, port, print, school, turf, lawn]  \n",
              "1               [fraenkel, victory, work, feel, year, well, trainer, air, cecil, battle, cancer, port]  \n",
              "2  [read, absolutely, brilliant, piece, sport, tribe, survive, cancer, suffer, rude, honor, privile...  \n",
              "3  [night, news, cancer, survivor, exercise, barrier, topic, dermatitis, bilingualism, stroke, spor...  \n",
              "4  [health, cancer, sport, inextricably, link, amazing, youngster, still, stay, play, soccer, diagn...  \n",
              "5                  [help, honor, step, cancer, journey, give, hope, lady, cancer, event, dubaiculture]  \n",
              "6                                                             [charity_ruin, cancer, research, baffle]  \n",
              "7                                             [oil, list, usa_drug_list, ff, cancer, cod, pain, sport]  \n",
              "8                                   [cheshirehour, look, charity, work, cancer, child, sport, contact]  \n",
              "9                   [seat, solution, bad, condition, train, give, sport, charity, cancer, fundraising]  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n",
        "    # Init output\n",
        "    sent_topics_df = pd.DataFrame()\n",
        "\n",
        "    # Get main topic in each document\n",
        "    for i, row_list in enumerate(ldamodel[corpus]):\n",
        "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
        "        # print(row)\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # => dominant topic\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                sent_topics_df = pd.concat([sent_topics_df, pd.DataFrame([int(topic_num), round(prop_topic,4), topic_keywords]).T], ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "\n",
        "    # Add original text to the end of the output\n",
        "    contents = pd.Series(texts)\n",
        "    contents = contents.reset_index(drop=True)\n",
        "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
        "    return(sent_topics_df)\n",
        "\n",
        "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready)\n",
        "\n",
        "# Format\n",
        "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
        "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
        "df_dominant_topic.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "4G6s5YBGxDtJ",
        "outputId": "db64d504-d064-44d1-cfb5-5fc91ec52951"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic_Num</th>\n",
              "      <th>Topic_Perc_Contrib</th>\n",
              "      <th>Keywords</th>\n",
              "      <th>Representative Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.7333</td>\n",
              "      <td>grow, thing, free, start, legal, long, illegal, trial, happy, first</td>\n",
              "      <td>[destroy, imagine, doctor, fired_hospital, close_pension, systems_brake, child, wait, long, inhe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.0667</td>\n",
              "      <td>hang, association, ebb, ply_watch, force, fool, thcnation, lead, let_parma, drops_daily</td>\n",
              "      <td>[speech, inspire, generation, athlete, sport, inspiration]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.8833</td>\n",
              "      <td>cancer, cure, medical, health, sure, medicine, tumour, treat, watch, benefit</td>\n",
              "      <td>[post, thought, cancer, bongfresh, bone, doctor, health, fitness, mmm, meditate, medical, fact, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.8963</td>\n",
              "      <td>cancer, study, treatment, thc, tumor, people, brain, reduce, mmj, show</td>\n",
              "      <td>[study, show, hour, period, fast, hour, increase, longevity, decrease, incidence, disease, inclu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.8445</td>\n",
              "      <td>cancer, great, drug, plant, time, die, extract, check, chemotherapy, food</td>\n",
              "      <td>[check, distance, time, time, sport, ultrarunne, cancer, chemotherapy]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>8</td>\n",
              "      <td>0.8444</td>\n",
              "      <td>cancer, patient, mme, find, life, journey, legalize, meet, science, become</td>\n",
              "      <td>[cancer, patient, credit, save, life, local, cancer]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>10</td>\n",
              "      <td>0.8833</td>\n",
              "      <td>cancer, cannabinoid, cell, kill, child, research, growth, pot, woman, inhibit</td>\n",
              "      <td>[cannabinoid, kill, cancer, cell, cell_proptosis, corner, pot]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>11</td>\n",
              "      <td>0.8444</td>\n",
              "      <td>help, cancer, day, new, cannabisoil, stop, fight, amp, rush, breakthetaboo</td>\n",
              "      <td>[give, fight, rare, form, cancer, fight, cancer]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>12</td>\n",
              "      <td>0.9067</td>\n",
              "      <td>cannabis, cancer, cod, oil, despoil, pain, cbd, weed, marijuana, epilepsy</td>\n",
              "      <td>[buy_nonthc_pure, cbd, oil, states_store, cod, cancer, cbdlife, cbdheal, cbdlife]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Topic_Num Topic_Perc_Contrib  \\\n",
              "0         0             0.7333   \n",
              "1         1             0.0667   \n",
              "2         2             0.8833   \n",
              "3         3             0.8963   \n",
              "4         4             0.8445   \n",
              "5         8             0.8444   \n",
              "6        10             0.8833   \n",
              "7        11             0.8444   \n",
              "8        12             0.9067   \n",
              "\n",
              "                                                                                  Keywords  \\\n",
              "0                      grow, thing, free, start, legal, long, illegal, trial, happy, first   \n",
              "1  hang, association, ebb, ply_watch, force, fool, thcnation, lead, let_parma, drops_daily   \n",
              "2             cancer, cure, medical, health, sure, medicine, tumour, treat, watch, benefit   \n",
              "3                   cancer, study, treatment, thc, tumor, people, brain, reduce, mmj, show   \n",
              "4                cancer, great, drug, plant, time, die, extract, check, chemotherapy, food   \n",
              "5               cancer, patient, mme, find, life, journey, legalize, meet, science, become   \n",
              "6            cancer, cannabinoid, cell, kill, child, research, growth, pot, woman, inhibit   \n",
              "7               help, cancer, day, new, cannabisoil, stop, fight, amp, rush, breakthetaboo   \n",
              "8                cannabis, cancer, cod, oil, despoil, pain, cbd, weed, marijuana, epilepsy   \n",
              "\n",
              "                                                                                   Representative Text  \n",
              "0  [destroy, imagine, doctor, fired_hospital, close_pension, systems_brake, child, wait, long, inhe...  \n",
              "1                                           [speech, inspire, generation, athlete, sport, inspiration]  \n",
              "2  [post, thought, cancer, bongfresh, bone, doctor, health, fitness, mmm, meditate, medical, fact, ...  \n",
              "3  [study, show, hour, period, fast, hour, increase, longevity, decrease, incidence, disease, inclu...  \n",
              "4                               [check, distance, time, time, sport, ultrarunne, cancer, chemotherapy]  \n",
              "5                                                 [cancer, patient, credit, save, life, local, cancer]  \n",
              "6                                       [cannabinoid, kill, cancer, cell, cell_proptosis, corner, pot]  \n",
              "7                                                     [give, fight, rare, form, cancer, fight, cancer]  \n",
              "8                    [buy_nonthc_pure, cbd, oil, states_store, cod, cancer, cbdlife, cbdheal, cbdlife]  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Display setting to show more characters in column\n",
        "pd.options.display.max_colwidth = 100\n",
        "\n",
        "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
        "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
        "\n",
        "for i, grp in sent_topics_outdf_grpd:\n",
        "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet,\n",
        "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)],\n",
        "                                            axis=0)\n",
        "\n",
        "# Reset Index\n",
        "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Format\n",
        "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
        "\n",
        "# Show\n",
        "sent_topics_sorteddf_mallet.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "f190_QZlxj9a"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 3)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[19], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m         topic_percentages\u001b[38;5;241m.\u001b[39mappend(topic_percs)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(dominant_topics, topic_percentages)\n\u001b[1;32m---> 13\u001b[0m dominant_topics, topic_percentages \u001b[38;5;241m=\u001b[39m \u001b[43mtopics_per_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlda_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Distribution of Dominant Topics in Each Document\u001b[39;00m\n\u001b[0;32m     16\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(dominant_topics, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDocument_Id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDominant_Topic\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
            "Cell \u001b[1;32mIn[19], line 7\u001b[0m, in \u001b[0;36mtopics_per_document\u001b[1;34m(model, corpus, start, end)\u001b[0m\n\u001b[0;32m      5\u001b[0m topic_percentages \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, corp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(corpus_sel):\n\u001b[1;32m----> 7\u001b[0m     topic_percs, wordid_topics, wordid_phivalues \u001b[38;5;241m=\u001b[39m model[corp]\n\u001b[0;32m      8\u001b[0m     dominant_topic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(topic_percs, key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      9\u001b[0m     dominant_topics\u001b[38;5;241m.\u001b[39mappend((i, dominant_topic))\n",
            "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 3)"
          ]
        }
      ],
      "source": [
        "# Sentence Coloring of N Sentences\n",
        "def topics_per_document(model, corpus, start=0, end=1):\n",
        "    corpus_sel = corpus[start:end]\n",
        "    dominant_topics = []\n",
        "    topic_percentages = []\n",
        "    for i, corp in enumerate(corpus_sel):\n",
        "        topic_percs, wordid_topics, wordid_phivalues = model[corp]\n",
        "        dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse=True)[0][0]\n",
        "        dominant_topics.append((i, dominant_topic))\n",
        "        topic_percentages.append(topic_percs)\n",
        "    return(dominant_topics, topic_percentages)\n",
        "\n",
        "dominant_topics, topic_percentages = topics_per_document(model=lda_model, corpus=corpus, end=-1)\n",
        "\n",
        "# Distribution of Dominant Topics in Each Document\n",
        "df = pd.DataFrame(dominant_topics, columns=['Document_Id', 'Dominant_Topic'])\n",
        "dominant_topic_in_each_doc = df.groupby('Dominant_Topic').size()\n",
        "df_dominant_topic_in_each_doc = dominant_topic_in_each_doc.to_frame(name='count').reset_index()\n",
        "\n",
        "# Total Topic Distribution by actual weight\n",
        "topic_weightage_by_doc = pd.DataFrame([dict(t) for t in topic_percentages])\n",
        "df_topic_weightage_by_doc = topic_weightage_by_doc.sum().to_frame(name='count').reset_index()\n",
        "\n",
        "# Top 3 Keywords for each Topic\n",
        "topic_top3words = [(i, topic) for i, topics in lda_model.show_topics(formatted=False)\n",
        "                                 for j, (topic, wt) in enumerate(topics) if j < 3]\n",
        "\n",
        "df_top3words_stacked = pd.DataFrame(topic_top3words, columns=['topic_id', 'words'])\n",
        "df_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\n",
        "df_top3words.reset_index(level=0,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "print(pd.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyLDAvis in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.24.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyLDAvis) (1.26.2)\n",
            "Requirement already satisfied: scipy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyLDAvis) (1.11.4)\n",
            "Requirement already satisfied: pandas>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyLDAvis) (2.2.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyLDAvis) (1.3.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyLDAvis) (3.1.3)\n",
            "Requirement already satisfied: numexpr in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyLDAvis) (2.8.7)\n",
            "Requirement already satisfied: funcy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyLDAvis) (2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyLDAvis) (1.4.1.post1)\n",
            "Requirement already satisfied: gensim in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyLDAvis) (4.3.2)\n",
            "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyLDAvis) (69.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.3.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim->pyLDAvis) (6.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->pyLDAvis) (2.1.5)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-ckaLggJxwya"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pyLDAvis.sklearn'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyLDAvis\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyLDAvis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m pyLDAvis\u001b[38;5;241m.\u001b[39menable_notebook()\n\u001b[0;32m      4\u001b[0m vis \u001b[38;5;241m=\u001b[39m pyLDAvis\u001b[38;5;241m.\u001b[39mgensim\u001b[38;5;241m.\u001b[39mprepare(lda_model, corpus, dictionary\u001b[38;5;241m=\u001b[39mlda_model\u001b[38;5;241m.\u001b[39mid2word)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis.sklearn'"
          ]
        }
      ],
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
        "vis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pyLDAvis.save_html(vis, 'lda_visualization.html')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ajouter une nouvelle colonne pour les étiquettes de sujets\n",
        "tweets_df['Topic_Label'] = \"\"\n",
        "\n",
        "# Mapper des noms de sujets aux identifiants de sujets\n",
        "topic_labels_mapping = {\n",
        "    0 : \"Traitement et recherche sur le cancer\",\n",
        "    1 : \"Combat contre le cancer et activisme\",\n",
        "    2 : \"Santé et bien-être liés au cancer\",\n",
        "    3 : \"Études cliniques et croissance tumorale\",\n",
        "    4 : \"Effets des cannabinoïdes sur le cancer\",\n",
        "    5 : \"Utilisation médicale du cannabis dans le traitement du cancer\",\n",
        "    6 : \"Impact environnemental du traitement du cancer\",\n",
        "    7 : \"Potentiel thérapeutique du cannabis dans la lutte contre le cancer\"\n",
        "}\n",
        "\n",
        "# Assigner des étiquettes en fonction des sujets dominants\n",
        "for index, row in df_dominant_topic.iterrows():\n",
        "    tweet_index = row['Document_No']\n",
        "    topic_id = row['Dominant_Topic']\n",
        "    topic_label = topic_labels_mapping[topic_id]\n",
        "    tweets_df.at[tweet_index, 'Topic_Label'] = topic_label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Spécifiez le chemin et le nom de fichier pour sauvegarder le DataFrame avec les étiquettes\n",
        "output_file_path = \"tweets_with_labels.csv\"\n",
        "\n",
        "# Enregistrez le DataFrame avec les étiquettes dans un fichier CSV\n",
        "tweets_df.to_csv(output_file_path, sep=';', index=False)\n",
        "\n",
        "# Confirmez que le fichier a été sauvegardé avec succès\n",
        "print(\"Le DataFrame avec les étiquettes a été sauvegardé dans :\", output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importez pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Lisez le fichier CSV dans un DataFrame\n",
        "t_df = pd.read_csv(\"tweets_with_labels.csv\", sep = \";\", skip_blank_lines=True)\n",
        "# Affichez la forme du DataFrame\n",
        "print(t_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "null_values = t_df[t_df['Topic_Label'].isnull()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "duplicates = t_df[t_df.duplicated(subset=['Topic_Label'], keep=False)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Supprimer les lignes avec des valeurs nulles dans la colonne \"Topic_Label\"\n",
        "#t_df = t_df.dropna(subset=['Topic_Label'])\n",
        "\n",
        "# Supprimer les doublons dans la colonne \"Topic_Label\"\n",
        "t_df = t_df.drop_duplicates(subset=['Fichier'], keep='first')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(t_df.shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
